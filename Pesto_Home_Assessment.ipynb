{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwfAeDjgAWE"
      },
      "source": [
        "# Data Engineering Case Study\n",
        "---------------\n",
        "\n",
        "Imagine you are a data engineer working for AdvertiseX, a digital advertising technology company. AdvertiseX specializes in programmatic advertising and manages multiple online advertising campaigns for its clients. The company handles vast amounts of data generated by ad impressions, clicks, conversions, and more. Your role as a data engineer is to address the following challenges:\n",
        "\n",
        "- ### Data Sources and Formats:\n",
        "\n",
        "  - #### Ad Impressions:\n",
        "\n",
        "    **Data Source:** AdvertiseX serves digital ads to various online platforms and websites. \\\n",
        "    **Data Format:** Ad impressions data is generated in JSON format, containing information such as ad creative ID, user ID, timestamp, and the website where the ad was displayed.\n",
        "\n",
        "  - #### Clicks and Conversions:\n",
        "\n",
        "    **Data Source:** AdvertiseX tracks user interactions with ads, including clicks and conversions (e.g., sign-ups, purchases). \\\n",
        "    **Data Format:** Click and conversion data is logged in CSV format and includes event timestamps, user IDs, ad campaign IDs, and conversion type.\n",
        "\n",
        "  - #### Bid Requests:\n",
        "\n",
        "    **Data Source:** AdvertiseX participates in real-time bidding (RTB) auctions to serve ads to users. \\\n",
        "    **Data Format:** Bid request data is received in a semi-structured format, mostly in Avro, and includes user information, auction details, and ad targeting criteria.\n",
        "\n",
        "- ### Case Study Requirements:\n",
        "\n",
        "  - #### Data Ingestion:\n",
        "\n",
        "    Implement a scalable data ingestion system capable of collecting and processing ad impressions (JSON), clicks/conversions (CSV), and bid requests (Avro) data. \\\n",
        "    Ensure that the ingestion system can handle high data volumes generated in real-time and batch modes.\n",
        "\n",
        "  - #### Data Processing:\n",
        "\n",
        "    Develop data transformation processes to standardize and enrich the data. Handle data validation, filtering, and deduplication. \\\n",
        "    Implement logic to correlate ad impressions with clicks and conversions to provide meaningful insights.\n",
        "\n",
        "  - #### Data Storage and Query Performance:\n",
        "\n",
        "    Select an appropriate data storage solution for storing processed data efficiently, enabling fast querying for campaign performance analysis. \\\n",
        "    Optimize the storage system for analytical queries and aggregations of ad campaign data.\n",
        "\n",
        "  - #### Error Handling and Monitoring:\n",
        "\n",
        "    Create an error handling and monitoring system to detect data anomalies, discrepancies, or delays. \\\n",
        "    Implement alerting mechanisms to address data quality issues in real-time, ensuring that discrepancies are resolved promptly to maintain ad campaign effectiveness.\n",
        "\n",
        "This Ad Tech case study scenario focuses on the challenges and data formats commonly encountered in the digital advertising industry. Candidates can use this information to design a data engineering solution that addresses the specific data processing and analysis needs of AdvertiseX.\n",
        "\n",
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUGAW6XQhtvs"
      },
      "source": [
        "\n",
        "# Solution\n",
        "---------------\n",
        "\n",
        "There are few ways to address the challenges mentioned above. However, based on different cloud platforms like GCP, AWS, etc. or Big data Platform (Hadoop), these service names may vary.   \n",
        "\n",
        "- ### Letâ€™s break down each requirement:\n",
        "\n",
        "    - #### Data Ingestion\n",
        "        - Scalable Data Ingestion System:\n",
        "            - Set up a robust data ingestion pipeline capable of handling high volumes of real-time and batch data.\n",
        "                - Google Pub/Sub, Apache Kafka or AWS Kinesis can be used for real-time streaming data ingestion.\n",
        "                - Use Apache Airflow for periodic batch data loads.\n",
        "        - Data Source-Specific Adapters:\n",
        "            - Develop custom adapters for each data source (ad impressions, clicks/conversions, bid requests) which can handle data retrieval, format conversion, and initial validation.\n",
        "            \n",
        "                We can use `json` library to parse json and `avro` library to parse avro file. However, pandas provide a high level abstraction to load them as dataframe as such I am using it in below example. Use `logging` module to keep track of errors and exceptions.\n",
        "\n",
        "                - Ingest Ad Impressions (JSON)\n",
        "                \n",
        "                    ```python\n",
        "                    import pandas as pd\n",
        "\n",
        "                    def ingest_ad_impressions(json_file_path):\n",
        "                        with open(json_file_path, 'r') as json_file:\n",
        "                            ad_impressions_data = pd.read_json(json_file)\n",
        "                            # Process and store ad impressions data (e.g., insert into a database)\n",
        "                            # Example: Insert into MongoDB or a relational database\n",
        "                            # ...\n",
        "\n",
        "                    # Usage\n",
        "                    json_file_path = 'path/to/ad_impressions.json'\n",
        "                    ingest_ad_impressions(json_file_path)\n",
        "\n",
        "                    ```\n",
        "\n",
        "                - Ingest Clicks and Conversions (CSV)\n",
        "\n",
        "                    ```python\n",
        "                    import pandas as pd\n",
        "\n",
        "                    def ingest_clicks_and_conversions(csv_file_path):\n",
        "                        click_conversion_df = pd.read_csv(csv_file_path)\n",
        "                        # Process and store click/conversion data (e.g., filter, validate, enrich)\n",
        "                        # Example: Validate timestamps, join with campaign details\n",
        "                        # ...\n",
        "\n",
        "                    # Usage\n",
        "                    csv_file_path = 'path/to/clicks_conversions.csv'\n",
        "                    ingest_clicks_and_conversions(csv_file_path)\n",
        "\n",
        "                    ```\n",
        "                \n",
        "                - Ingest Bid Requests (Avro)\n",
        "\n",
        "                    ```python\n",
        "                    import pandas as pd\n",
        "                    from fastavro import reader\n",
        "\n",
        "                    def ingest_bid_requests(avro_file_path):\n",
        "                        with open(avro_file_path, 'rb') as avro_file:\n",
        "                            avro_df = pd.DataFrame(reader(avro_file))\n",
        "                            # Process and store bid request data (e.g., extract user info, auction details)\n",
        "                            # Example: Extract user demographics, validate auction details\n",
        "                            # ...\n",
        "\n",
        "                    # Usage\n",
        "                    avro_file_path = 'path/to/bid_requests.avro'\n",
        "                    ingest_bid_requests(avro_file_path)\n",
        "\n",
        "                    ```\n",
        "\n",
        "    - #### Data Processing\n",
        "        - Standardization and Enrichment:\n",
        "            - Transform raw data into a common schema for consistency.\n",
        "                - Convert timestamps to a consistent format (e.g., ISO 8601)\n",
        "                - Handle missing values (e.g., fill or drop)\n",
        "                - Standardize column names (e.g., rename columns)\n",
        "            - Enrich data by adding relevant metadata (e.g., campaign details, user demographics, Top of Funnel (TOF), Bottom of Funnel (BOF), conversion ratio, ).\n",
        "            - Use tools like Apache Spark or Databricks for scalable data processing.\n",
        "        - Correlation Logic:\n",
        "            - Join ad impressions with clicks and conversions based on common identifiers (e.g., user ID, creative ID).\n",
        "            - Calculate click-through rates (CTR) and conversion rates.\n",
        "            - Identify successful campaigns and optimize targeting strategies.\n",
        "                \n",
        "                ```python\n",
        "                    def correlate_impressions_with_clicks(impressions_df, clicks_df):\n",
        "                        # Assuming both DataFrames have common identifiers (e.g., user ID, creative ID)\n",
        "                        # Merge ad impressions with clicks\n",
        "                        correlated_df = impressions_df.merge(clicks_df, on='user_id', how='left')\n",
        "                        return correlated_df\n",
        "\n",
        "                    # Usage\n",
        "                    correlated_data_df = correlate_impressions_with_clicks(ad_impressions_df, clicks_df)\n",
        "                ```\n",
        "\n",
        "        - Data Validation and Deduplication:\n",
        "            - Implement validation rules (e.g., timestamp consistency, valid user IDs).\n",
        "\n",
        "                ```python\n",
        "                    def validate_data(data):\n",
        "                        # Check timestamp consistency (e.g., clicks after impressions)\n",
        "                        # Validate user IDs (e.g., non-negative integers)\n",
        "                        # Example: Using pandas for validation\n",
        "                        valid_data = data[data['event_timestamp'].notnull() & (data['user_id'] >= 0)]\n",
        "                        return valid_data\n",
        "\n",
        "                    # Usage\n",
        "                    validated_data_df = validate_data(correlated_data_df)\n",
        "                ```\n",
        "\n",
        "            - Remove duplicate records to ensure data accuracy.\n",
        "                \n",
        "                ```python\n",
        "                    def remove_duplicates(data):\n",
        "                        # Drop duplicate rows based on specific columns\n",
        "                        deduplicated_data = data.drop_duplicates(subset=['user_id', 'event_timestamp'])\n",
        "                        return deduplicated_data\n",
        "\n",
        "                    # Usage\n",
        "                    deduplicated_bid_requests_df = remove_duplicates(bid_requests_df)\n",
        "                ```\n",
        "\n",
        "    - #### Data Storage and Query Performance\n",
        "        - Storage Solution Selection:\n",
        "            - Choose an appropriate storage system based on requirements:\n",
        "                - **Data Warehouse:** For structured data (e.g., clicks/conversions), use solutions like Amazon Redshift, Google BigQuery, or Snowflake. Below is sample code to query with Bigquery with python client library.\n",
        "\n",
        "                ```python\n",
        "                    from google.cloud import bigquery\n",
        "\n",
        "                    def query_bigquery():\n",
        "                        client = bigquery.Client()\n",
        "\n",
        "                        # Define your SQL query\n",
        "                        query = \"\"\"\n",
        "                        SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013`\n",
        "                        WHERE state = 'TX'\n",
        "                        LIMIT 100\n",
        "                        \"\"\"\n",
        "\n",
        "                        # Execute the query\n",
        "                        query_job = client.query(query)\n",
        "                        rows = query_job.result()\n",
        "\n",
        "                        # Print the results\n",
        "                        for row in rows:\n",
        "                            print(row.name)\n",
        "\n",
        "                    # Usage\n",
        "                    query_bigquery()\n",
        "                ```\n",
        "\n",
        "                - **NoSQL Databases:** For semi-structured data (e.g., ad impressions), consider MongoDB, Cassandra, or Elasticsearch. Below is sample code to save data into MongoDB.\n",
        "                \n",
        "                ```python\n",
        "\n",
        "                    from pymongo import MongoClient\n",
        "\n",
        "                    def store_ad_impressions(dataframe):\n",
        "                        client = MongoClient('mongodb://localhost:27017/')\n",
        "                        db = client['advertising_db']\n",
        "                        collection = db['ad_impressions']\n",
        "\n",
        "                        # Insert data into MongoDB\n",
        "                        collection.insert_many(dataframe.to_dict('records'))\n",
        "\n",
        "                        # Close the connection\n",
        "                        client.close()\n",
        "\n",
        "                    # Usage\n",
        "                    store_ad_impressions(ad_impressions_df)\n",
        "                ```\n",
        "\n",
        "                - **Object Storage:** Store raw data (e.g., Avro files) in Amazon S3 or Google Cloud Storage (GCS). Below is sample code to save data into GCS.\n",
        "\n",
        "                ```python\n",
        "                    from google.cloud import storage\n",
        "\n",
        "                    def upload_to_gcs(local_file_path, bucket_name, object_name):\n",
        "                        storage_client = storage.Client()\n",
        "                        bucket = storage_client.bucket(bucket_name)\n",
        "                        blob = bucket.blob(object_name)\n",
        "\n",
        "                        # Upload the local file to GCS\n",
        "                        blob.upload_from_filename(local_file_path)\n",
        "\n",
        "                    # Usage\n",
        "                    local_file_path = 'path/to/local_file.avro'\n",
        "                    bucket_name = 'your-gcs-bucket'\n",
        "                    object_name = 'ad_impressions.avro'\n",
        "                    upload_to_gcs(local_file_path, bucket_name, object_name)\n",
        "                ```\n",
        "\n",
        "        - Optimization for Analytical Queries\n",
        "\n",
        "            - Create materialized views in BigQuery to pre-aggregate data for common queries (e.g., daily campaign performance).\n",
        "            - Define the view using SQL and schedule refresh intervals.\n",
        "            - Partition tables by hourly, daily, monthly, yearly (e.g., daily or hourly partitions) to improve query efficiency.\n",
        "            - Use Clustering for field which have categorical data. \n",
        "            - Use ingestion-time partitioning for streaming data.\n",
        "\n",
        "    - #### Error Handling and Monitoring\n",
        "\n",
        "        - Set up monitoring tools (e.g., Grafana, or Google Cloud Monitoring) and create data quality dashboard to track data quality metrics (e.g., data completeness, latency).\n",
        "        - Monitor data pipelines and identify bottlenecks or failures.\n",
        "        - Implement automated retries for failed data ingestion or processing tasks.\n",
        "        - Detect anomalies (e.g., sudden drops in impressions, high CTR) using statistical methods.\n",
        "        - Trigger alerts via email, Slack, or SMS when discrepancies occur.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
